# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11qENM5xbbbqWysB1ZPhIcO1y3yP-ZbWf
"""

!pip install datasets

import pandas as pd
import torch
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from datasets import Dataset
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

def load_assamese_dataset(file_path):
    if file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
    elif file_path.endswith(('.xlsx', '.xls')):
        df = pd.read_excel(file_path)
    else:
        raise ValueError("File must be CSV or Excel format")

    print("Available columns:", df.columns)  # Debugging step

    assamese_col = 'asm'
    english_col = 'eng'

    return df, assamese_col, english_col

def prepare_assamese_dataset(df, assamese_col, english_col, n_shots=100):
    translations = [{"asm": row[assamese_col], "eng": row[english_col]} for _, row in df.iterrows()]
    train_data, test_data = train_test_split(translations, test_size=0.2, random_state=42)
    return train_data[:n_shots], test_data

def tokenize_dataset(translations, tokenizer, max_length=128):
    def preprocess_function(examples):
        inputs = tokenizer(examples['asm'], padding='max_length', truncation=True, max_length=max_length)
        targets = tokenizer(examples['eng'], padding='max_length', truncation=True, max_length=max_length)
        return {"input_ids": inputs["input_ids"], "attention_mask": inputs["attention_mask"], "labels": targets["input_ids"]}

    dataset = Dataset.from_pandas(pd.DataFrame(translations))
    return dataset.map(preprocess_function, batched=True)

def translate_text(model, tokenizer, text, direction):
    tokenizer_lang_codes = tokenizer.lang_code_to_id.keys()
    lang_map = {'as2en': ('bn_IN', 'en_XX'), 'en2as': ('en_XX', 'bn_IN')}  # Updated mapping for Assamese

    if direction not in lang_map:
        return "Invalid translation direction"

    src_lang, tgt_lang = lang_map[direction]

    if src_lang not in tokenizer_lang_codes or tgt_lang not in tokenizer_lang_codes:
        return f"Language code error: {src_lang} or {tgt_lang} not found in tokenizer. Available codes: {tokenizer_lang_codes}"

    tokenizer.src_lang = src_lang
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang], max_length=128)
    return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]

def main():
    print("Upload your Assamese-English dataset file...")
    from google.colab import files
    uploaded = files.upload()
    file_name = list(uploaded.keys())[0]
    df, assamese_col, english_col = load_assamese_dataset(file_name)

    tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
    model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

    print("Tokenizer supported languages:", tokenizer.lang_code_to_id.keys())

    train_translations, test_translations = prepare_assamese_dataset(df, assamese_col, english_col, n_shots=100)
    train_dataset = tokenize_dataset(train_translations, tokenizer)

    while True:
        text = input("Enter text for translation (or 'exit' to stop): ")
        if text.lower() == 'exit':
            break

        direction = input("Enter 'as2en' for Assamese to English or 'en2as' for English to Assamese: ")
        translation = translate_text(model, tokenizer, text, direction)
        print("Translation:", translation)

    return train_dataset, model, tokenizer

if __name__ == "__main__":
    train_dataset, model, tokenizer = main()


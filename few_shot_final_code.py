# -*- coding: utf-8 -*-
"""Few Shot Final Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HBOPW071OX1r_pEJ8dZP4jfpwg-HTpBA
"""

# -*- coding: utf-8 -*-
"""
Enhanced Few-shot multilingual translation using mBART + LoRA
Fixed: Language code validation, array length matching, error handling
"""

# âœ… INSTALL LIBRARIES
!pip install transformers datasets peft accelerate sacrebleu torch -q

# Check transformers version for compatibility
import transformers
print(f"ğŸ”§ Transformers version: {transformers.__version__}")

# âœ… IMPORT LIBRARIES
import torch
import os
from datasets import Dataset
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
import json
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

# âœ… ENHANCED DATASET WITH VALID LANGUAGE CODES ONLY
def create_training_data():
    """Create expanded training dataset with valid mBART language codes"""
    # Valid Indian language codes in mBART-50
    valid_langs = ["hi_IN", "bn_IN", "ta_IN", "te_IN", "mr_IN", "gu_IN"]

    data = {
        "src_text": [],
        "tgt_text": [],
        "tgt_lang": []
    }

    # Training phrases
    phrases = [
        ("How are you?", ["à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?", "à¦†à¦ªà¦¨à¦¿ à¦•à§‡à¦®à¦¨ à¦†à¦›à§‡à¦¨?", "à®¨à¯€à®™à¯à®•à®³à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?",
                         "à°®à±€à°°à± à°à°²à°¾ à°‰à°¨à±à°¨à°¾à°°à±?", "à¤¤à¥à¤®à¥à¤¹à¥€ à¤•à¤¸à¥‡ à¤†à¤¹à¤¾à¤¤?", "àª¤àª®à«‡ àª•à«‡àª® àª›à«‹?"]),

        ("Thank you", ["à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦", "à¦§à¦¨à§à¦¯à¦¬à¦¾à¦¦", "à®¨à®©à§à®±à®¿", "à°§à°¨à±à°¯à°µà°¾à°¦à°¾à°²à±", "à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦", "àª†àª­àª¾àª°"]),

        ("Good morning", ["à¤¸à¥à¤ªà¥à¤°à¤­à¤¾à¤¤", "à¦¸à§à¦ªà§à¦°à¦­à¦¾à¦¤", "à®•à®¾à®²à¯ˆ à®µà®£à®•à¯à®•à®®à¯", "à°¶à±à°­à±‹à°¦à°¯à°‚", "à¤¸à¥à¤ªà¥à¤°à¤­à¤¾à¤¤", "àª¸à«àªªà«àª°àª­àª¾àª¤"]),

        ("What is your name?", ["à¤†à¤ªà¤•à¤¾ à¤¨à¤¾à¤® à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?", "à¦†à¦ªà¦¨à¦¾à¦° à¦¨à¦¾à¦® à¦•à¦¿?", "à®‰à®™à¯à®•à®³à¯ à®ªà¯†à®¯à®°à¯ à®à®©à¯ à¤¨?",
                               "à°®à±€ à°ªà±‡à°°à± à°à°®à°¿à°Ÿà°¿?", "à¤¤à¥à¤®à¤šà¥‡ à¤¨à¤¾à¤µ à¤•à¤¾à¤¯ à¤†à¤¹à¥‡?", "àª¤àª®àª¾àª°à«àª‚ àª¨àª¾àª® àª¶à«àª‚ àª›à«‡?"]),

        ("I am fine", ["à¤®à¥ˆà¤‚ à¤ à¥€à¤• à¤¹à¥‚à¤", "à¦†à¦®à¦¿ à¦­à¦¾à¦²à§‹ à¦†à¦›à¦¿", "à®¨à®¾à®©à¯ à®¨à®²à®®à®¾à®• à®‡à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯",
                      "à°¨à±‡à°¨à± à°¬à°¾à°—à±à°¨à±à°¨à°¾à°¨à±", "à¤®à¥€ à¤¬à¤°à¤¾ à¤†à¤¹à¥‡", "àª¹à«àª‚ àª¸àª¾àª°à«‹ àª›à«àª‚"]),

        ("Good night", ["à¤¶à¥à¤­ à¤°à¤¾à¤¤à¥à¤°à¤¿", "à¦¶à§à¦­ à¦°à¦¾à¦¤à§à¦°à¦¿", "à®‡à®©à®¿à®¯ à®‡à®°à®µà¯", "à°¶à±à°­ à°°à°¾à°¤à±à°°à°¿", "à¤¶à¥à¤­ à¤°à¤¾à¤¤à¥à¤°à¥€", "àª¶à«àª­ àª°àª¾àª¤à«àª°àª¿"]),

        ("Where are you?", ["à¤†à¤ª à¤•à¤¹à¤¾à¤ à¤¹à¥ˆà¤‚?", "à¦†à¦ªà¦¨à¦¿ à¦•à§‹à¦¥à¦¾à¦¯à¦¼?", "à®¨à¯€à®™à¯à®•à®³à¯ à®à®™à¯à®•à¯‡ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?",
                           "à°®à±€à°°à± à°à°•à±à°•à°¡ à°‰à°¨à±à°¨à°¾à°°à±?", "à¤¤à¥à¤®à¥à¤¹à¥€ à¤•à¥à¤ à¥‡ à¤†à¤¹à¤¾à¤¤?", "àª¤àª®à«‡ àª•à«àª¯àª¾àª‚ àª›à«‹?"]),

        ("I love you", ["à¤®à¥ˆà¤‚ à¤¤à¥à¤®à¤¸à¥‡ à¤ªà¥à¤¯à¤¾à¤° à¤•à¤°à¤¤à¤¾ à¤¹à¥‚à¤", "à¦†à¦®à¦¿ à¦¤à§‹à¦®à¦¾à¦•à§‡ à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¿", "à®¨à®¾à®©à¯ à®‰à®©à¯à®©à¯ˆ à®•à®¾à®¤à®²à®¿à®•à¯à®•à®¿à®±à¯‡à®©à¯",
                       "à°¨à±‡à°¨à± à°¨à°¿à°¨à±à°¨à± à°ªà±à°°à±‡à°®à°¿à°¸à±à°¤à±à°¨à±à°¨à°¾à°¨à±", "à¤®à¥€ à¤¤à¥à¤à¥à¤¯à¤¾à¤µà¤° à¤ªà¥à¤°à¥‡à¤® à¤•à¤°à¤¤à¥‹", "àª¹à«àª‚ àª¤àª¨à«‡ àªªà«àª°à«‡àª® àª•àª°à«àª‚ àª›à«àª‚"])
    ]

    # Create balanced dataset
    for src_phrase, tgt_phrases in phrases:
        for i, lang_code in enumerate(valid_langs):
            data["src_text"].append(src_phrase)
            data["tgt_text"].append(tgt_phrases[i])
            data["tgt_lang"].append(lang_code)

    print(f"ğŸ“Š Created dataset with {len(data['src_text'])} examples")
    print(f"ğŸŒ Languages: {', '.join(valid_langs)}")

    return Dataset.from_dict(data)

# âœ… VALIDATE LANGUAGE CODES
def validate_language_codes(tokenizer):
    """Check which language codes are available in the tokenizer"""
    available_langs = list(tokenizer.lang_code_to_id.keys())
    indian_langs = [lang for lang in available_langs if lang.endswith('_IN')]

    print("ğŸ” Available Indian language codes in mBART-50:")
    for lang in indian_langs:
        print(f"  - {lang}")

    return indian_langs

# âœ… LOAD TOKENIZER & MODEL WITH ERROR HANDLING
def load_model_and_tokenizer(model_name: str = "facebook/mbart-large-50-many-to-many-mmt"):
    """Load model and tokenizer with proper error handling"""
    try:
        print(f"Loading tokenizer from {model_name}...")
        tokenizer = MBart50TokenizerFast.from_pretrained(model_name)

        print(f"Loading model from {model_name}...")
        model = MBartForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        print(f"âœ… Model loaded successfully! Using device: {next(model.parameters()).device}")

        # Validate language codes
        validate_language_codes(tokenizer)

        return tokenizer, model

    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        raise

# âœ… ENHANCED LoRA CONFIGURATION
def setup_lora(model):
    """Setup LoRA configuration with better parameters"""
    lora_config = LoraConfig(
        r=16,  # Increased rank for better performance
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "out_proj"],  # More modules
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM
    )

    peft_model = get_peft_model(model, lora_config)

    # Get correct parameter counts
    total_params = sum(p.numel() for p in peft_model.parameters())
    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)

    print(f"âœ… LoRA configured.")
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    print(f"ğŸ¯ Trainable parameters: {trainable_params:,}")
    print(f"ğŸ“ˆ Trainable %: {100 * trainable_params / total_params:.2f}%")

    return peft_model

# âœ… FIXED PREPROCESSING WITH PROPER VALIDATION
def create_preprocess_function(tokenizer, max_length: int = 128):
    """Create preprocessing function with fixed error handling"""
    tokenizer.src_lang = "en_XX"

    def preprocess(examples):
        try:
            # Tokenize source text
            model_inputs = tokenizer(
                examples["src_text"],
                max_length=max_length,
                truncation=True,
                padding="max_length"
            )

            labels = []
            valid_indices = []

            # Process each example and track valid ones
            for idx, (tgt_text, tgt_lang) in enumerate(zip(examples["tgt_text"], examples["tgt_lang"])):
                # Verify language code exists
                if tgt_lang not in tokenizer.lang_code_to_id:
                    print(f"âš ï¸ Skipping invalid language code: {tgt_lang}")
                    continue

                tokenizer.tgt_lang = tgt_lang
                with tokenizer.as_target_tokenizer():
                    tokenized_label = tokenizer(
                        tgt_text,
                        max_length=max_length,
                        truncation=True,
                        padding="max_length"
                    )
                    label_ids = tokenized_label["input_ids"]
                    # Replace padding tokens with -100 for loss calculation
                    label_ids = [label if label != tokenizer.pad_token_id else -100 for label in label_ids]
                    labels.append(label_ids)
                    valid_indices.append(idx)

            # Filter model inputs to match valid labels
            if len(valid_indices) != len(examples["src_text"]):
                print(f"âš ï¸ Filtered {len(examples['src_text']) - len(valid_indices)} invalid examples")
                # Filter all input arrays to match valid indices
                for key in model_inputs.keys():
                    model_inputs[key] = [model_inputs[key][i] for i in valid_indices]

            model_inputs["labels"] = labels

            # Verify all arrays have same length
            lengths = [len(v) for v in model_inputs.values()]
            if len(set(lengths)) > 1:
                raise ValueError(f"Mismatched array lengths: {lengths}")

            return model_inputs

        except Exception as e:
            print(f"âŒ Error in preprocessing: {e}")
            raise

    return preprocess

# âœ… TRAINING WITH BETTER CONFIGURATION
def train_model(model, tokenizer, dataset, output_dir: str = "./mbart-finetuned-multilang"):
    """Train model with enhanced configuration"""

    # Split dataset for training and validation
    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = train_test_split['train']
    eval_dataset = train_test_split['test']

    # Preprocess datasets
    preprocess_fn = create_preprocess_function(tokenizer)
    train_tokenized = train_dataset.map(preprocess_fn, batched=True, remove_columns=dataset.column_names)
    eval_tokenized = eval_dataset.map(preprocess_fn, batched=True, remove_columns=dataset.column_names)

    # Training arguments with version compatibility
    try:
        # Try newer parameter name first
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=2,  # Reduced for stability
            per_device_eval_batch_size=2,
            learning_rate=3e-4,  # Slightly higher for LoRA
            num_train_epochs=8,  # Reduced epochs for stability
            save_total_limit=2,
            logging_steps=5,
            eval_steps=10,
            save_strategy="epoch",
            eval_strategy="epoch",  # For newer transformers versions
            fp16=torch.cuda.is_available(),
            report_to="none",
            label_names=["labels"],
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            warmup_steps=10,
            gradient_accumulation_steps=2,
            dataloader_pin_memory=False  # Avoid memory issues
        )
    except TypeError:
        # Fallback to older parameter name
        print("âš ï¸ Using older transformers version, falling back to 'evaluation_strategy'")
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            learning_rate=3e-4,
            num_train_epochs=8,
            save_total_limit=2,
            logging_steps=5,
            eval_steps=10,
            save_strategy="epoch",
            evaluation_strategy="epoch",  # For older transformers versions
            fp16=torch.cuda.is_available(),
            report_to="none",
            label_names=["labels"],
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            warmup_steps=10,
            gradient_accumulation_steps=2,
            dataloader_pin_memory=False
        )

    # Data collator
    data_collator = DataCollatorForSeq2Seq(tokenizer, model, padding=True)

    # Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        eval_dataset=eval_tokenized,
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    print("ğŸš€ Starting training...")
    trainer.train()
    print("âœ… Training completed!")

    return trainer

# âœ… ENHANCED TRANSLATION FUNCTION
class MultilingualTranslator:
    def __init__(self, model_path: str = "./mbart-finetuned-multilang"):
        # 7 low-resource language codes (6 Indian + Nepali)
        self.lang_codes = {
            "Hindi": "hi_IN",
            "Bengali": "bn_IN",
            "Tamil": "ta_IN",
            "Telugu": "te_IN",
            "Marathi": "mr_IN",
            "Gujarati": "gu_IN",
            "Nepali": "ne_NP"  # Added Nepali as 7th low-resource language
        }
        self.load_model(model_path)

    def load_model(self, model_path: str):
        """Load the fine-tuned model"""
        try:
            if os.path.exists(model_path):
                print(f"Loading fine-tuned model from {model_path}...")
                self.tokenizer = MBart50TokenizerFast.from_pretrained(model_path)
                self.model = MBartForConditionalGeneration.from_pretrained(model_path)
            else:
                print("Fine-tuned model not found. Loading base model...")
                self.tokenizer, self.model = load_model_and_tokenizer()

            print("âœ… Model loaded for inference!")
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            raise

    def translate_to_language(self, text: str, target_lang: str) -> str:
        """Translate text to a specific language"""
        try:
            self.tokenizer.src_lang = "en_XX"
            encoded = self.tokenizer(text, return_tensors="pt", max_length=128, truncation=True)

            lang_code = self.lang_codes.get(target_lang)
            if not lang_code:
                return f"âŒ Unsupported language: {target_lang}"

            forced_bos_token_id = self.tokenizer.lang_code_to_id.get(lang_code)
            if forced_bos_token_id is None:
                return f"âŒ Language code not found: {lang_code}"

            with torch.no_grad():
                output = self.model.generate(
                    **encoded,
                    forced_bos_token_id=forced_bos_token_id,
                    max_length=128,
                    num_beams=5,
                    early_stopping=True,
                    do_sample=False
                )

            translation = self.tokenizer.decode(output[0], skip_special_tokens=True)
            return translation

        except Exception as e:
            return f"âŒ Translation error: {e}"

    def translate_to_all(self, text: str) -> Dict[str, str]:
        """Translate text to all supported languages"""
        results = {}
        for lang in self.lang_codes.keys():
            results[lang] = self.translate_to_language(text, lang)
        return results

    def save_translations(self, text: str, translations: Dict[str, str], filename: str = "translations.json"):
        """Save translations to file"""
        import datetime
        data = {
            "original_text": text,
            "translations": translations,
            "timestamp": datetime.datetime.now().isoformat()
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Translations saved to {filename}")

# âœ… MAIN EXECUTION
def main():
    """Main execution function"""
    print("ğŸŒ Enhanced Multilingual Translation System")
    print("=" * 50)
    print("ğŸ“ Supports 7 low-resource languages with mBART-50")
    print("ğŸ”— Supported languages: Hindi, Bengali, Tamil, Telugu, Marathi, Gujarati, Nepali")
    print("=" * 50)

    try:
        # Create dataset
        print("ğŸ“Š Creating training dataset...")
        dataset = create_training_data()
        print(f"âœ… Dataset created with {len(dataset)} examples")

        # Load model and tokenizer
        tokenizer, model = load_model_and_tokenizer()

        # Setup LoRA
        model = setup_lora(model)

        # Train model
        trainer = train_model(model, tokenizer, dataset)

        # Save model
        print("ğŸ’¾ Saving fine-tuned model...")
        trainer.model.save_pretrained("./mbart-finetuned-multilang")
        tokenizer.save_pretrained("./mbart-finetuned-multilang")
        print("âœ… Model saved successfully!")

        # Initialize translator
        translator = MultilingualTranslator("./mbart-finetuned-multilang")

        # Interactive translation
        print("\nğŸŒ Interactive Translation")
        print("Supported Languages:", ", ".join(translator.lang_codes.keys()))
        print("-" * 50)

        while True:
            user_input = input("\nEnter English text to translate (or 'quit' to exit): ").strip()

            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break

            if not user_input:
                print("âš ï¸ Please enter some text.")
                continue

            print("\nğŸ”„ Translating...")
            results = translator.translate_to_all(user_input)

            print(f"\nğŸ“ Original: {user_input}")
            print("ğŸŒ Translations:")
            print("-" * 30)

            for lang, translation in results.items():
                print(f"{lang:10}: {translation}")

            # Option to save
            save_option = input("\nğŸ’¾ Save translations? (y/n): ").strip().lower()
            if save_option == 'y':
                translator.save_translations(user_input, results)

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Process interrupted by user")
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()

# âœ… RUN THE PROGRAM
if __name__ == "__main__":
    main()
# -*- coding: utf-8 -*-
"""Few Shot Final Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HBOPW071OX1r_pEJ8dZP4jfpwg-HTpBA
"""

# -*- coding: utf-8 -*-
"""
Enhanced Few-shot multilingual translation using mBART + LoRA
Fixed: Language code validation, array length matching, error handling
"""

# ‚úÖ INSTALL LIBRARIES
!pip install transformers datasets peft accelerate sacrebleu torch -q

# Check transformers version for compatibility
import transformers
print(f"üîß Transformers version: {transformers.__version__}")

# ‚úÖ IMPORT LIBRARIES
import torch
import os
from datasets import Dataset
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
import json
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

# ‚úÖ ENHANCED DATASET WITH VALID LANGUAGE CODES ONLY
def create_training_data():
    """Create expanded training dataset with valid mBART language codes"""
    # Valid Indian language codes in mBART-50
    valid_langs = ["hi_IN", "bn_IN", "ta_IN", "te_IN", "mr_IN", "gu_IN"]

    data = {
        "src_text": [],
        "tgt_text": [],
        "tgt_lang": []
    }

    # Training phrases
    phrases = [
        ("How are you?", ["‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?", "‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡ßá‡¶Æ‡¶® ‡¶Ü‡¶õ‡ßá‡¶®?", "‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç?",
                         "‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?", "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§ï‡§∏‡•á ‡§Ü‡§π‡§æ‡§§?", "‡™§‡™Æ‡´á ‡™ï‡´á‡™Æ ‡™õ‡´ã?"]),

        ("Thank you", ["‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶", "‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶", "‡Æ®‡Æ©‡ßç‡Æ±‡Æø", "‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å", "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶", "‡™Ü‡™≠‡™æ‡™∞"]),

        ("Good morning", ["‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§", "‡¶∏‡ßÅ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶§", "‡Æï‡Ææ‡Æ≤‡Øà ‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç", "‡∞∂‡±Å‡∞≠‡±ã‡∞¶‡∞Ø‡∞Ç", "‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§", "‡™∏‡´Å‡™™‡´ç‡™∞‡™≠‡™æ‡™§"]),

        ("What is your name?", ["‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?", "‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶ï‡¶ø?", "‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç ‡Æé‡Æ©‡Øç ‡§®?",
                               "‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?", "‡§§‡•Å‡§Æ‡§ö‡•á ‡§®‡§æ‡§µ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?", "‡™§‡™Æ‡™æ‡™∞‡´Å‡™Ç ‡™®‡™æ‡™Æ ‡™∂‡´Å‡™Ç ‡™õ‡´á?"]),

        ("I am fine", ["‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å", "‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶õ‡¶ø", "‡Æ®‡Ææ‡Æ©‡Øç ‡Æ®‡Æ≤‡ÆÆ‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç",
                      "‡∞®‡±á‡∞®‡±Å ‡∞¨‡∞æ‡∞ó‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å", "‡§Æ‡•Ä ‡§¨‡§∞‡§æ ‡§Ü‡§π‡•á", "‡™π‡´Å‡™Ç ‡™∏‡™æ‡™∞‡´ã ‡™õ‡´Å‡™Ç"]),

        ("Good night", ["‡§∂‡•Å‡§≠ ‡§∞‡§æ‡§§‡•ç‡§∞‡§ø", "‡¶∂‡ßÅ‡¶≠ ‡¶∞‡¶æ‡¶§‡ßç‡¶∞‡¶ø", "‡Æá‡Æ©‡Æø‡ÆØ ‡Æá‡Æ∞‡Æµ‡ØÅ", "‡∞∂‡±Å‡∞≠ ‡∞∞‡∞æ‡∞§‡±ç‡∞∞‡∞ø", "‡§∂‡•Å‡§≠ ‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä", "‡™∂‡´Å‡™≠ ‡™∞‡™æ‡™§‡´ç‡™∞‡™ø"]),

        ("Where are you?", ["‡§Ü‡§™ ‡§ï‡§π‡§æ‡§Å ‡§π‡•à‡§Ç?", "‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º?", "‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æô‡Øç‡Æï‡Øá ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç?",
                           "‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞ï‡±ç‡∞ï‡∞° ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?", "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§ï‡•Å‡§†‡•á ‡§Ü‡§π‡§æ‡§§?", "‡™§‡™Æ‡´á ‡™ï‡´ç‡™Ø‡™æ‡™Ç ‡™õ‡´ã?"]),

        ("I love you", ["‡§Æ‡•à‡§Ç ‡§§‡•Å‡§Æ‡§∏‡•á ‡§™‡•ç‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Å", "‡¶Ü‡¶Æ‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø", "‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æ©‡Øç‡Æ©‡Øà ‡Æï‡Ææ‡Æ§‡Æ≤‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç",
                       "‡∞®‡±á‡∞®‡±Å ‡∞®‡∞ø‡∞®‡±ç‡∞®‡±Å ‡∞™‡±ç‡∞∞‡±á‡∞Æ‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å", "‡§Æ‡•Ä ‡§§‡•Å‡§ù‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§™‡•ç‡§∞‡•á‡§Æ ‡§ï‡§∞‡§§‡•ã", "‡™π‡´Å‡™Ç ‡™§‡™®‡´á ‡™™‡´ç‡™∞‡´á‡™Æ ‡™ï‡™∞‡´Å‡™Ç ‡™õ‡´Å‡™Ç"])
    ]

    # Create balanced dataset
    for src_phrase, tgt_phrases in phrases:
        for i, lang_code in enumerate(valid_langs):
            data["src_text"].append(src_phrase)
            data["tgt_text"].append(tgt_phrases[i])
            data["tgt_lang"].append(lang_code)

    print(f"üìä Created dataset with {len(data['src_text'])} examples")
    print(f"üåç Languages: {', '.join(valid_langs)}")

    return Dataset.from_dict(data)

# ‚úÖ VALIDATE LANGUAGE CODES
def validate_language_codes(tokenizer):
    """Check which language codes are available in the tokenizer"""
    available_langs = list(tokenizer.lang_code_to_id.keys())
    indian_langs = [lang for lang in available_langs if lang.endswith('_IN')]

    print("üîç Available Indian language codes in mBART-50:")
    for lang in indian_langs:
        print(f"  - {lang}")

    return indian_langs

# ‚úÖ LOAD TOKENIZER & MODEL WITH ERROR HANDLING
def load_model_and_tokenizer(model_name: str = "facebook/mbart-large-50-many-to-many-mmt"):
    """Load model and tokenizer with proper error handling"""
    try:
        print(f"Loading tokenizer from {model_name}...")
        tokenizer = MBart50TokenizerFast.from_pretrained(model_name)

        print(f"Loading model from {model_name}...")
        model = MBartForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        print(f"‚úÖ Model loaded successfully! Using device: {next(model.parameters()).device}")

        # Validate language codes
        validate_language_codes(tokenizer)

        return tokenizer, model

    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        raise

# ‚úÖ ENHANCED LoRA CONFIGURATION
def setup_lora(model):
    """Setup LoRA configuration with better parameters"""
    lora_config = LoraConfig(
        r=16,  # Increased rank for better performance
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "out_proj"],  # More modules
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM
    )

    peft_model = get_peft_model(model, lora_config)

    # Get correct parameter counts
    total_params = sum(p.numel() for p in peft_model.parameters())
    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)

    print(f"‚úÖ LoRA configured.")
    print(f"üìä Total parameters: {total_params:,}")
    print(f"üéØ Trainable parameters: {trainable_params:,}")
    print(f"üìà Trainable %: {100 * trainable_params / total_params:.2f}%")

    return peft_model

# ‚úÖ FIXED PREPROCESSING WITH PROPER VALIDATION
def create_preprocess_function(tokenizer, max_length: int = 128):
    """Create preprocessing function with fixed error handling"""
    tokenizer.src_lang = "en_XX"

    def preprocess(examples):
        try:
            # Tokenize source text
            model_inputs = tokenizer(
                examples["src_text"],
                max_length=max_length,
                truncation=True,
                padding="max_length"
            )

            labels = []
            valid_indices = []

            # Process each example and track valid ones
            for idx, (tgt_text, tgt_lang) in enumerate(zip(examples["tgt_text"], examples["tgt_lang"])):
                # Verify language code exists
                if tgt_lang not in tokenizer.lang_code_to_id:
                    print(f"‚ö†Ô∏è Skipping invalid language code: {tgt_lang}")
                    continue

                tokenizer.tgt_lang = tgt_lang
                with tokenizer.as_target_tokenizer():
                    tokenized_label = tokenizer(
                        tgt_text,
                        max_length=max_length,
                        truncation=True,
                        padding="max_length"
                    )
                    label_ids = tokenized_label["input_ids"]
                    # Replace padding tokens with -100 for loss calculation
                    label_ids = [label if label != tokenizer.pad_token_id else -100 for label in label_ids]
                    labels.append(label_ids)
                    valid_indices.append(idx)

            # Filter model inputs to match valid labels
            if len(valid_indices) != len(examples["src_text"]):
                print(f"‚ö†Ô∏è Filtered {len(examples['src_text']) - len(valid_indices)} invalid examples")
                # Filter all input arrays to match valid indices
                for key in model_inputs.keys():
                    model_inputs[key] = [model_inputs[key][i] for i in valid_indices]

            model_inputs["labels"] = labels

            # Verify all arrays have same length
            lengths = [len(v) for v in model_inputs.values()]
            if len(set(lengths)) > 1:
                raise ValueError(f"Mismatched array lengths: {lengths}")

            return model_inputs

        except Exception as e:
            print(f"‚ùå Error in preprocessing: {e}")
            raise

    return preprocess

# ‚úÖ TRAINING WITH BETTER CONFIGURATION
def train_model(model, tokenizer, dataset, output_dir: str = "./mbart-finetuned-multilang"):
    """Train model with enhanced configuration"""

    # Split dataset for training and validation
    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = train_test_split['train']
    eval_dataset = train_test_split['test']

    # Preprocess datasets
    preprocess_fn = create_preprocess_function(tokenizer)
    train_tokenized = train_dataset.map(preprocess_fn, batched=True, remove_columns=dataset.column_names)
    eval_tokenized = eval_dataset.map(preprocess_fn, batched=True, remove_columns=dataset.column_names)

    # Training arguments with version compatibility
    try:
        # Try newer parameter name first
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=2,  # Reduced for stability
            per_device_eval_batch_size=2,
            learning_rate=3e-4,  # Slightly higher for LoRA
            num_train_epochs=8,  # Reduced epochs for stability
            save_total_limit=2,
            logging_steps=5,
            eval_steps=10,
            save_strategy="epoch",
            eval_strategy="epoch",  # For newer transformers versions
            fp16=torch.cuda.is_available(),
            report_to="none",
            label_names=["labels"],
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            warmup_steps=10,
            gradient_accumulation_steps=2,
            dataloader_pin_memory=False  # Avoid memory issues
        )
    except TypeError:
        # Fallback to older parameter name
        print("‚ö†Ô∏è Using older transformers version, falling back to 'evaluation_strategy'")
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            learning_rate=3e-4,
            num_train_epochs=8,
            save_total_limit=2,
            logging_steps=5,
            eval_steps=10,
            save_strategy="epoch",
            evaluation_strategy="epoch",  # For older transformers versions
            fp16=torch.cuda.is_available(),
            report_to="none",
            label_names=["labels"],
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            warmup_steps=10,
            gradient_accumulation_steps=2,
            dataloader_pin_memory=False
        )

    # Data collator
    data_collator = DataCollatorForSeq2Seq(tokenizer, model, padding=True)

    # Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        eval_dataset=eval_tokenized,
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    print("üöÄ Starting training...")
    trainer.train()
    print("‚úÖ Training completed!")

    return trainer

# ‚úÖ ENHANCED TRANSLATION FUNCTION
class MultilingualTranslator:
    def __init__(self, model_path: str = "./mbart-finetuned-multilang"):
        # 7 low-resource language codes (6 Indian + Nepali)
        self.lang_codes = {
            "Hindi": "hi_IN",
            "Bengali": "bn_IN",
            "Tamil": "ta_IN",
            "Telugu": "te_IN",
            "Marathi": "mr_IN",
            "Gujarati": "gu_IN",
            "Nepali": "ne_NP"  # Added Nepali as 7th low-resource language
        }
        self.load_model(model_path)

    def load_model(self, model_path: str):
        """Load the fine-tuned model"""
        try:
            if os.path.exists(model_path):
                print(f"Loading fine-tuned model from {model_path}...")
                self.tokenizer = MBart50TokenizerFast.from_pretrained(model_path)
                self.model = MBartForConditionalGeneration.from_pretrained(model_path)
            else:
                print("Fine-tuned model not found. Loading base model...")
                self.tokenizer, self.model = load_model_and_tokenizer()

            print("‚úÖ Model loaded for inference!")
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            raise

    def translate_to_language(self, text: str, target_lang: str) -> str:
        """Translate text to a specific language"""
        try:
            self.tokenizer.src_lang = "en_XX"
            encoded = self.tokenizer(text, return_tensors="pt", max_length=128, truncation=True)

            lang_code = self.lang_codes.get(target_lang)
            if not lang_code:
                return f"‚ùå Unsupported language: {target_lang}"

            forced_bos_token_id = self.tokenizer.lang_code_to_id.get(lang_code)
            if forced_bos_token_id is None:
                return f"‚ùå Language code not found: {lang_code}"

            with torch.no_grad():
                output = self.model.generate(
                    **encoded,
                    forced_bos_token_id=forced_bos_token_id,
                    max_length=128,
                    num_beams=5,
                    early_stopping=True,
                    do_sample=False
                )

            translation = self.tokenizer.decode(output[0], skip_special_tokens=True)
            return translation

        except Exception as e:
            return f"‚ùå Translation error: {e}"

    def translate_to_all(self, text: str) -> Dict[str, str]:
        """Translate text to all supported languages"""
        results = {}
        for lang in self.lang_codes.keys():
            results[lang] = self.translate_to_language(text, lang)
        return results

    def save_translations(self, text: str, translations: Dict[str, str], filename: str = "translations.json"):
        """Save translations to file"""
        import datetime
        data = {
            "original_text": text,
            "translations": translations,
            "timestamp": datetime.datetime.now().isoformat()
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ Translations saved to {filename}")

# ‚úÖ MAIN EXECUTION
def main():
    """Main execution function"""
    print("üåç Enhanced Multilingual Translation System")
    print("=" * 50)
    print("üìù Supports 7 low-resource languages with mBART-50")
    print("üîó Supported languages: Hindi, Bengali, Tamil, Telugu, Marathi, Gujarati, Nepali")
    print("=" * 50)

    try:
        # Create dataset
        print("üìä Creating training dataset...")
        dataset = create_training_data()
        print(f"‚úÖ Dataset created with {len(dataset)} examples")

        # Load model and tokenizer
        tokenizer, model = load_model_and_tokenizer()

        # Setup LoRA
        model = setup_lora(model)

        # Train model
        trainer = train_model(model, tokenizer, dataset)

        # Save model
        print("üíæ Saving fine-tuned model...")
        trainer.model.save_pretrained("./mbart-finetuned-multilang")
        tokenizer.save_pretrained("./mbart-finetuned-multilang")
        print("‚úÖ Model saved successfully!")

        # Initialize translator
        translator = MultilingualTranslator("./mbart-finetuned-multilang")

        # Interactive translation
        print("\nüåê Interactive Translation")
        print("Supported Languages:", ", ".join(translator.lang_codes.keys()))
        print("-" * 50)

        while True:
            user_input = input("\nEnter English text to translate (or 'quit' to exit): ").strip()

            if user_input.lower() in ['quit', 'exit', 'q']:
                print("üëã Goodbye!")
                break

            if not user_input:
                print("‚ö†Ô∏è Please enter some text.")
                continue

            print("\nüîÑ Translating...")
            results = translator.translate_to_all(user_input)

            print(f"\nüìù Original: {user_input}")
            print("üåê Translations:")
            print("-" * 30)

            for lang, translation in results.items():
                print(f"{lang:10}: {translation}")

            # Option to save
            save_option = input("\nüíæ Save translations? (y/n): ").strip().lower()
            if save_option == 'y':
                translator.save_translations(user_input, results)

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Process interrupted by user")
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()

# ‚úÖ RUN THE PROGRAM
if __name__ == "__main__":
    main()